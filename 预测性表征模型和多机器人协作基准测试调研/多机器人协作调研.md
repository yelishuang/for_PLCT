# 多机器人协作调研报告

---

## 第一部分：Meta-World基准调研

### 概述
 
Meta-World是一个专门用于多任务和元强化学习的开源模拟基准测试平台，由斯坦福大学、加州大学伯克利分校等机构的研究人员共同开发，并在2019年机器人学习会议（CoRL 2019）上正式发表。该基准包含50个不同的机器人操作任务，旨在推动多任务学习和元强化学习算法的发展。通过设计多样化且具有共享结构的任务，Meta-World使算法能够有效泛化到新任务。

### 1. Meta-World基准设计

#### 1.1 任务构成

**任务数量与类型**
Meta-World涵盖50种机器人操作任务，包括抓取、推动、放置物体等常见操作行为，具体任务如开门、关闭抽屉、按压按钮等。

**变异性设计**
- **参数化变异性**：在每个任务内部，物体位置和目标位置随机变化。例如，在"抓取物体并放置到目标位置"的任务中，每次任务开始时物体和目标的位置都会发生变化，增加任务多样性。
- **非参数化变异性**：不同任务之间存在本质差异。如"抓取物体"和"打开窗户"具有完全不同的操作方式和目标，这种设计确保了任务的丰富性和挑战性。

#### 1.2 环境与空间设置

**机器人平台**
所有任务均使用模拟的Sawyer机械臂作为执行平台。

**动作空间**
机械臂的动作空间由以下部分组成：
- 机械臂末端执行器的3D位置变化
- 夹爪(gripper)的扭矩控制

动作空间维度固定，便于算法开发与调试。开发者通过控制末端执行器在三维空间中的位移（x、y、z向量）以及夹爪施加的扭矩来执行任务。

**观测空间**
观测空间维度固定为39维，包含以下信息：
- 末端执行器坐标和归一化后的夹持器张开程度
- 主要对象信息（坐标+四元数）
- 可能存在的第二个对象信息（坐标+四元数）
- 环境预设观测信息（场景中固定障碍物位置、光照强度归一化值等）
- 目标坐标

#### 1.3 奖励函数设计

Meta-World将复杂任务分解为几个关键步骤：到达(reach)、抓取(grasp)、放置(place)。每完成一个步骤都会获得相应奖励，所有任务的奖励满分均为10分，确保训练过程的平衡性。这种分阶段奖励机制使算法能够逐步学习完成复杂任务。

### 2. Meta-World评估协议

Meta-World提供多种评估模式，适用于不同类型的研究和算法测试：

#### 2.1 ML1（单任务适应）
专注于在单一任务内适应新的目标位置。例如在"reach"任务中，目标位置不断变化，算法需要快速适应不同的目标位置，验证算法在同一任务下对不同目标的泛化能力。

#### 2.2 MT10和MT50（多任务学习）
- **MT10**：要求学习一个策略来解决10个训练任务
- **MT50**：要求解决50个训练任务

在这些任务中，对象和目标位置相对固定，重点考察算法学习多个任务的能力，即训练一个通用策略来应对多种不同任务。

#### 2.3 ML10和ML45（元学习）
- **ML10**：在10个训练任务上进行元训练，然后在新的测试任务上快速适应
- **ML45**：在45个训练任务上进行元训练，再在新任务上测试

在这两种模式下，不提供任务ID，更好地考验算法在面对未知任务时的快速学习和适应能力。

### 3. 使用Meta-World的实施步骤

#### 3.1 安装环境

```bash
pip install metaworld
```

#### 3.2 选择评估协议与任务

**确定评估目标**
- 测试单一任务内不同目标适应能力：选择ML1
- 评估多任务学习能力：选择MT10或MT50
- 关注新任务快速适应能力：选择ML10或ML45

**任务选择**
从50个任务中选择符合评估需求的任务。例如，研究抓取和放置类任务表现时，可选择"sawyer_pick_place_v2"等相关任务，或根据评估协议要求选择相应数量和类型的任务组合。

#### 3.3 模型训练与测试流程

**算法集成**
将模型或方法集成到Meta-World环境中。根据Meta-World提供的API接口，编写代码使模型能够接收环境观测信息并输出相应动作指令。

**训练过程**
按选定的评估协议进行训练，监控模型性能指标（如成功率、奖励获取情况等）。根据训练结果调整模型超参数或优化算法以提高性能。

**测试评估**
完成训练后，在相应测试任务集上对模型进行测试。记录模型在不同任务上的性能表现，与其他算法在Meta-World上的结果进行对比分析。

### 4. 典型应用案例

#### 4.1 Model-Agnostic Meta-Learning (MAML)
MAML提出模型无关的元学习框架，通过学习良好的初始化参数，使算法能够快速适应新任务。在ML45基准上，MAML解决了约39.9%的元测试任务，明显优于RL²的33.3%。

#### 4.2 Meta-Zero + Reconstruction (MZ+Recon)
该方法结合零样本学习和重构机制，在45任务元学习场景中展现出优异性能，在Meta-World最具挑战性的ML45基准上超越了MAML等经典方法。

#### 4.3 Reinforcement Learning Squared (RL²)
虽然性能被MAML超越，但在ML45上仍能解决33.3%的元测试任务，证明了基于循环神经网络的元学习方法在复杂机器人操作任务中的有效性，为后续基于RNN的元学习方法奠定了基础。

#### 4.4 Probabilistic Embeddings for Actor-Critic RL (PEARL)
通过概率嵌入来编码任务信息，使智能体能够快速识别和适应新任务。在Meta-World的少样本学习场景中验证了概率建模在元强化学习中的优势。

### 5. 总结与局限性

Meta-World是专门为单个机器人的多任务和元强化学习设计的基准测试平台，包含50个不同的机器人操作任务。其架构专注于单一机械臂的操作技能学习，50个操作任务具有多样性但共享结构，便于进行高效的多任务强化学习和元强化学习的任务迁移。需要注意的是，Meta-World的设计更适合单个智能体的技能泛化研究，而不适用于多智能体协调场景。对于多机器人协作研究，需要考虑其他专门设计的基准测试平台或对现有框架进行扩展。

---

## 第二部分：CALVIN基准调研

### 概述

CALVIN（Composing Actions from Language and Vision）是一个专门用于长视距机器人操作任务的语言条件策略学习基准。该基准由斯坦福大学等机构开发，旨在解决机器人通过自然语言指令执行复杂操作任务的问题。CALVIN填补了现有基准在语言驱动长序列机器人操作方面的空白，为研究机器人语言理解和执行能力提供了标准化平台。

### 1.基准设计

#### 1.1 环境构成

**环境设置**
CALVIN包含四个结构相关但纹理和布局不同的室内操作环境（A、B、C、D），每个环境配备：
- 7-DOF Franka Emika Panda机械臂
- 带滑动门和抽屉的桌子
- 控制灯光的按钮和开关
- 不同颜色和形状的矩形块

环境基于PyBullet物理引擎构建，支持GPU加速渲染，便于大规模并行数据收集。

**传感器配置**
- RGB-D图像：来自静态相机和末端执行器相机
- 本体感受信息：关节位置、夹爪宽度等
- 基于视觉的触觉传感

**动作控制**
支持三种控制方式：绝对坐标、相对坐标（相对于夹爪坐标系）、关节空间控制

#### 1.2 数据集构成

**演示数据**
通过HTC Vive VR设备收集了24小时的远程操作数据，每个环境约6小时（约240万交互步骤）。操作员在无任务约束下自由探索环境，生成包含物体拾取放置、开关抽屉、按按钮等各种动作的自然交互数据。

**语言指令数据**
- 超过20K条自然语言指令，对应34种具体任务
- 仅对1%的记录数据进行语言标注，模拟现实场景
- 提供MiniLM提取的384维语言嵌入向量

#### 1.3 任务定义

CALVIN定义了34种具体任务，主要类型包括：

**物体移动任务**
"将蓝色方块向左推"：要求物体移动超过10厘米且保持与表面接触

**开关操作任务**
"打开抽屉"：要求抽屉拉出至少10厘米

**物体抓取任务**
"提起红色方块"：要求从桌面抓取并提升至少5厘米

**长视距任务序列**
支持将基础任务组合成最长5步的任务序列，如"打开抽屉→拾取蓝色方块→将方块推入抽屉→打开滑动门"

### 2. CALVIN评估协议

#### 2.1 训练测试环境组合
支持多种训练测试方式：
- 在单个环境中训练并评估，对应传统任务学习场景
- 在所有四个环境（A,B,C,D）中训练，在其中一个环境中评估，考验对不同纹理和布局的泛化能力
- 在三个环境中(A,B,C)训练，在未见过的第四个环境(D)中评估，是最具挑战性的设置

#### 2.2 评估指标

**多任务语言控制（MTLC）**
评估智能体对34种操作任务的泛化能力：
- 每个任务从有效但未见过的演示状态开始
- 使用训练集中未出现的新语言指令进行测试
- 每个任务执行10次取平均成功率

**长视距MTLC（LH-MTLC）**
测试连续执行多个语言指令的能力：
- 将34个任务作为子目标，生成由5个连续任务组成的有效序列
- 过滤后得到1000条唯一指令链
- 评估了不同传感器组合（包括静态相机RGB图像、夹爪相机RGB-D图像、触觉传感等）下的智能体表现

### 3. 使用CALVIN的实施步骤

#### 3.1 环境安装
```bash
# 克隆仓库
git clone --recurse-submodules https://github.com/mees/calvin.git
export CALVIN_ROOT=$(pwd)/calvin

# 安装依赖
cd $CALVIN_ROOT
conda create -n calvin_venv python=3.8
conda activate calvin_venv
sh install.sh
```

#### 3.2 下载数据集
```bash
cd $CALVIN_ROOT/dataset
# 选择一个数据集规模下载
sh download_data.sh debug    # 调试用小数据集(1.3GB)
# sh download_data.sh D      # 单环境
# sh download_data.sh ABC    # 三环境
# sh download_data.sh ABCD   # 四环境(完整)
```

#### 3.3 训练模型

**基础训练**
```bash
cd $CALVIN_ROOT/calvin_models/calvin_agent
python training.py datamodule.root_data_dir=/path/to/dataset/ datamodule/datasets=vision_lang_shm
```

**常用配置**
```bash
# 多GPU训练
python training.py trainer.gpus=-1

# RGB双摄像头
python training.py datamodule/observation_space=lang_rgb_static_gripper_rel_act model/perceptual_encoder=gripper_cam

# RGB-D双摄像头
python training.py datamodule/observation_space=lang_rgbd_both model/perceptual_encoder=RGBD_both
```

### 4. 典型应用案例

#### 4.1 Predictive Inverse Dynamics Models are Scalable Learners for Robotic Manipulation (2024)
该论文提出了预测逆动力学模型(Seer)，通过预测下一个状态来学习逆动力学。在CALVIN基准上实现了显著的性能提升，在D→D设置中达到了98.2%的成功率，在更具挑战性的ABC→D跨环境泛化设置中达到了95.7%的成功率，展现了预测建模在机器人操作中的强大能力。

#### 4.2 Diffusion Transformer Policy: Scaling Diffusion Transformer for Generalist Vision-Language-Action Learning (2024)
该方法将扩散模型与Transformer架构结合，提出了可扩展的视觉-语言-动作学习框架。在CALVIN基准测试中验证了大规模扩散模型在长期任务序列中的优势，通过渐进式生成策略有效处理了复杂的多步骤机器人操作任务，在多环境泛化场景中表现出色。

#### 4.3 GR-MG: Leveraging Partially Annotated Data via Multi-Modal Goal Conditioned Policy (2024)  
该论文提出了利用部分标注数据的多模态目标条件策略，通过巧妙的数据利用策略降低了对完全标注数据的依赖。在CALVIN基准上达到了当前最先进的性能，在D→D设置中成功率超过97%，同时在数据效率方面相比传统方法提升了40%以上，证明了半监督学习在机器人操作中的潜力。

#### 4.4 GHIL-Glue: Hierarchical Control with Filtered Subgoal Images (2024)
该方法提出了基于过滤子目标图像的分层控制框架，利用预训练的图像生成模型来指导机器人的分层决策。在CALVIN基准测试中验证了分层控制在长期任务规划中的有效性，特别是在处理包含多个子任务的复杂指令序列时，相比端到端方法在任务完成率上提升了15-20%。

### 5. 总结与局限性

CALVIN是一个专门用于评估长时间序列语言条件控制策略学习的基准测试，主要设计用于"单个智能体"解决复杂操作任务。从目前的文献来看，CALVIN主要关注的是单机器人在理解一系列语言指令后执行长期操作任务的能力。


## 第三部分：多机器人协作

### 1. 多机器人协作测试基准的现状
目前，针对多机器人协作的标准测试基准仍不够完善。已有部分研究围绕多机器人协作感知与 SLAM、路径规划与导航等方面展开，但许多论文在测试方法上仍沿用各自领域的传统做法，甚至采用非开源的数据集，缺乏统一和可比较的评估体系。
#### 1.1 多机器人路径规划领域的测试基准问题
从多机器人路径规划的角度来看，“Multi-Agent Path Finding with Continuous Time”（ICAPS 2019）一文提出了一种基于可满足性模理论（SMT）的 SMT-CBS^R 算法，旨在求解具有最小 makespan 的最优路径。该研究采用了分层图结构作为测试基准，其中类似 [2,2]、[3,1,3] 这样的表述代表不同的图结构特征。例如，[2,2] 表示该图包含两层，每层有两个顶点；而 [3,1,3] 则表示三层结构，第一层有三个顶点，第二层一个顶点，第三层三个顶点。整个图结构层数范围为 2 到 5 层，每层最多包含 5 个顶点，且相邻层之间为全连接结构。在测试流程中，智能体从第一层的起始位置出发，以最后一层的目标位置为终点，起始与目标配置均为随机排列，智能体直径设定为 0.2。通过运行时间、决策次数和成功率等指标来评估算法性能。然而，这种测试基准显然存在样本量不足、场景设置科学性不强的问题，难以全面反映算法的实际效果。

相比之下，“Scalable Multi-Robot Motion Planning for Congested Environments”（RSS 2020）提出了一种适用于拥挤环境的可扩展多机器人运动规划方法 Composite Dynamic Region-biased RRT（CDR-RRT）。该方法将单机器人拓扑引导规划策略拓展至多机器人系统，通过构建复合空间骨架和动态区域采样策略，提升复杂狭窄通道环境下的规划效率与可扩展性。具体贡献包括：设计基于各机器人工作空间骨架笛卡尔积的复合骨架结构；结合动态区域偏置采样与多智能体路径搜索启发式算法引导骨架扩展；并通过保留全局随机采样机制确保概率完备性。为验证其有效性，论文设置了多种具有代表性的测试场景，包括狭窄通道中的走廊场景（机器人需交换位置或单向通行）、入口场景（一方机器人需主动避让进入侧方入口）、轨道场景（环形路径中多机器人同向移动避免碰撞），以及仓库类多通道环境、开放交叉环境和三维迷宫隧道等复杂地形。尽管这些测试场景相比前一篇论文更具多样性，但仍属于作者自定义的小样本测试，缺乏广泛适用性和统计显著性。

总体来看，当前多机器人路径规划领域仍缺乏具备足够样本量和科学性的标准化测试基准。
#### 1.2 多机器人协作感知与 SLAM 的测试基准挑战
在多机器人协作感知与 SLAM 方面，“Towards Collaborative Simultaneous Localization and Mapping: a Survey of the Current Research Landscape” 综述指出，C-SLAM（协作同步定位与建图）作为多机器人 SLAM 的关键技术，在自动驾驶车队、工业应用等领域展现出巨大潜力。其核心在于整合多机器人数据以构建全局一致的地图与定位信息，但也面临通信约束、计算资源限制及动态环境适应等挑战。当前研究已逐步从实验室走向实际应用，如 DARPA 地下挑战赛中的部署案例，并呈现出分布式优化、语义融合、边缘计算集成等趋势。

在测试基准方面，C-SLAM 缺乏统一的评估标准。现有方法多依赖于拆分单机器人数据集（如 KITTI、KITTI360、Pit30M）来模拟多机器人场景，但需注意避免非现实的重叠区域设置。近年来，一些专用数据集如 AirMuseum（异构多机器人）和 DARPA SubT（含标准化工具）逐渐兴起。其中，AirMuseum 数据集专为多机器人立体视觉与惯性 SLAM 设计，包含五个由地面与空中机器人在法国 ONERA 默东的一个前航空博物馆内采集的室内协同 SLAM 场景。每个场景包含多个机器人之间的同步序列，涵盖立体图像和惯性测量单元（IMU）数据，并通过 AprilTag 标记检测实现显式的机器人间交互。真实轨迹通过 Structure-from-Motion 算法结合固定信标标记进行约束计算得出。该数据集是开源的，但仍存在样本数量有限的问题。
#### 1.3 多智能体深度强化学习相关测试框架
此外，“Multi-Agent Deep Reinforcement Learning for Cooperative Multi-Robot Navigation”（Nature Machine Intelligence 2023）指出，随着现实任务复杂度的提升，单一机器人难以独立完成，多机器人协作需求日益迫切。多智能体深度强化学习（MADRL）凭借其处理大规模状态空间的能力，使机器人能够学习彼此间的交互策略，从而适应动态环境。文中提到的 VMAS 是一个开源的 2D 物理模拟器，基于 PyTorch 实现向量化计算，专为高效的多智能体强化学习基准测试而设计。其物理引擎支持并行仿真，无需额外复杂性即可在加速硬件上运行。VMAS 提供十二种具有挑战性的多机器人场景，接口模块化，便于研究人员添加新场景。

MultiRoboLearn 是一个基于 ROS 的多机器人 DRL 框架，支持仿真实验与真实机器人实验，尤其侧重于导航任务。ROS 提供了丰富的工具与库，极大简化了感知、导航、操作和协调等复杂任务的开发流程。

MARLlib 是一个基于 Ray 的多智能体强化学习库，提供了如 PPO、DDPG 等多种基线算法，具备强大的大规模机器人集群仿真能力，例如支持多达 1024 台机器人进行路径规划等复杂任务。

OpenAI MPE 在多机器人协作研究中也具有一定影响力。它提供了一系列标准化的多智能体任务场景，包括简单的导航、追逐、通信和协作任务，环境具有连续的状态和动作空间，支持部分可观测性和智能体间的物理交互。MPE 基于二维粒子世界构建精细的物理模拟系统，每个智能体被建模为具有质量、惯性等物理属性的圆形粒子实体，能够发生真实的弹性碰撞与推挤交互。